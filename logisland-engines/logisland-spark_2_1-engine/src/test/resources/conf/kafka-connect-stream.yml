version: 0.12.2
documentation: LogIsland future factory job

engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  type: engine
  documentation: Index some apache logs with logisland
  configuration:
    spark.app.name: FutureFactory
    spark.master: local[*]
    spark.driver.memory: 512M
    spark.driver.cores: 1
    spark.executor.memory: 512M
    spark.executor.instances: 4
    spark.executor.cores: 2
    spark.yarn.queue: default
    spark.yarn.maxAppAttempts: 4
    spark.yarn.am.attemptFailuresValidityInterval: 1h
    spark.yarn.max.executor.failures: 20
    spark.yarn.executor.failuresValidityInterval: 1h
    spark.task.maxFailures: 8
    spark.serializer: org.apache.spark.serializer.KryoSerializer
   #spark.serializer: org.apache.spark.serializer.JavaSerializer
    spark.streaming.batchDuration: 2000
    spark.streaming.backpressure.enabled: false
    spark.streaming.blockInterval: 500
    spark.streaming.kafka.maxRatePerPartition: 10000
    spark.streaming.timeout: -1
    spark.streaming.unpersist: false
    spark.streaming.kafka.maxRetries: 3
    spark.streaming.ui.retainedBatches: 200
    spark.streaming.receiver.writeAheadLog.enable: false
    spark.ui.port: 4040

  controllerServiceConfigurations:

    - controllerService: kc_source_service
      component: com.hurence.logisland.stream.spark.structured.provider.KafkaConnectStructuredProviderService
      configuration:
        kc.data.value.converter: com.hurence.logisland.util.kafkaconnect.LogIslandRecordConverter
        kc.data.value.converter.properties: |
          record.serializer=com.hurence.logisland.serializer.KryoSerializer
        kc.data.key.converter.properties: |
          schemas.enable=false
        kc.data.key.converter: org.apache.kafka.connect.json.JsonConverter
        kc.worker.tasks.max: 1
        kc.connector.class: com.hurence.logisland.engine.kc.BinarySourceConnector
        kc.connector.properties: |
          schema.name=test
          topic=test
          filename.path=/Users/amarziali/nasa.txt
    - controllerService: console_service
      component: com.hurence.logisland.stream.spark.structured.provider.ConsoleStructuredStreamProviderService

  streamConfigurations:

    # indexing stream
    - stream: indexing_stream
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.topics: /a/in
        read.topics.serializer: com.hurence.logisland.serializer.KryoSerializer
        read.topics.key.serializer: com.hurence.logisland.serializer.JsonSerializer
        read.topics.client.service: kc_source_service
        write.topics: /a/out
        write.topics.serializer: none
        write.topics.client.service: console_service

      processorConfigurations:


        - processor: flatten
          component: com.hurence.logisland.processor.FlatMap
          type: processor
          documentation: "extract from root record"
          configuration:
            keep.root.record: false
            copy.root.record.fields: true
            leaf.record.type: record_value

        - processor: create_aliases
          component: com.hurence.logisland.processor.NormalizeFields
          type: processor
          documentation: "creates an alias tagname/record_name. record_name will be used as metric name into Chronix"
          configuration:
            conflict.resolution.policy: do_nothing
            record_name: name
            record_value: binary
            record_key: name

        - processor: stream_debugger
          component: com.hurence.logisland.processor.DebugStream
          type: processor
          documentation: debug records
          configuration:
            event.serializer: json